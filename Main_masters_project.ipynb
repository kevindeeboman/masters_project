{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project imports ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2 as pg2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import calendar\n",
    "from calendar import monthrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project overview\n",
    "#### Objective: Create a historic daily labor demand index based of total available jobs for a given day between 2006-2020      \n",
    "- In this project, only job ads from Arbetsf√∂rmedlingen (JobTechDev) will be used. In a future project, current labor demand index will be constructed and supplemented with ads through the means of web scraping.\n",
    "    \n",
    "- Statistics from the European Commission[<sup>1</sup>](#fn1) shows that the percentage of job seekers using public employment services in EU-countries is among the highest in Sweden, above 70%. Thus, the index may be a good complement to official statistics in measuring labor demand.\n",
    "    \n",
    "- Performace of the index will be evaluated through comparison to Swedens official statistics (survey index).\n",
    "\n",
    "#### Project steps\n",
    "- Step 1: Converting JSON files to CSV and extracting relevant data (+data cleaning).\n",
    "- Step 2: Inserting data into a postgreSQL database and setting up relevant query.\n",
    "- Step 3: visualization and discussion of results.\n",
    "- Setp 4: Statistical analysis time series decomposition using Loess (In progress).\n",
    "\n",
    "<span id=\"fn1\"> footnote 1</span>: European Commission (2017) [European Semester Thematic Factsheet](https://wayback.archive-it.org/12090/20201012083437/https://ec.europa.eu/info/sites/info/files/european-semester_thematic-factsheet_public-employment-services_en_0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Converting JSON files to CSV and extracting relevant data (+ data cleaning)\n",
    "#### All job ads data used in this project is from [JobTechDev](https://jobtechdev.se/docs/apis/historical/), an initiative by the Swedish public employment service.\n",
    "- The complete dataset is about 30.8 GB and files are in the JSON-format\n",
    "- Dataset contains 32 differerent columns/vars, many of which are beyond the scope of the project, as such a selection will be extracted.\n",
    "\n",
    "- Variables to be extracted for this project are the following:\n",
    "    - headline: The ad headline\n",
    "    - number_of_vacancies: The number of advertised jobs for ad\n",
    "    - publication_date: Date the ad was published on the job ad platform\n",
    "    - application_deadline: The last date to apply for the job\n",
    "    - last_publication_date: The last date the ad was public, used as substitutet for application deadline for 2017 where application_deadline is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid = 0; total_errors = 0\n",
    "for file in range(2006,2021):\n",
    "    filename = f'/Users/Kevin/Desktop/project_dta/json_pb2006_2020/{file}.json'\n",
    "    with open(filename) as f:\n",
    "        ads = json.load(f)\n",
    "        file_ads = []\n",
    "        error_rows = 0\n",
    "        \n",
    "    # For 2017 application_deadline is null, last_publication_date used as a proxy, vars 99.8% equivalent #\n",
    "    for ad in ads:\n",
    "        # Removal of special characters using regex, events of \\n causes errors in csv file #\n",
    "        head_line = re.sub('[!,*)@#%(&$_?.^\\\\\\\\\\n/]', '', str(ad['headline']))\n",
    "        if file != 2017:\n",
    "            try:\n",
    "                ad_select = [head_line, ad['number_of_vacancies'], ad['publication_date'][:10], ad['application_deadline'][:10]]\n",
    "            except:\n",
    "                error_rows += 1\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                ad_select = [head_line, ad['number_of_vacancies'], ad['publication_date'][:10], ad['last_publication_date'][:10]]    \n",
    "            except:\n",
    "                error_rows += 1\n",
    "                continue\n",
    "                \n",
    "        # Jobs ads with no vacancies (=0) or missing values (None) in dates or vacancies are removed #\n",
    "        if all(ad_select[1:]):\n",
    "            file_ads.append(ad_select)\n",
    "        else:\n",
    "            error_rows += 1\n",
    "            continue\n",
    "\n",
    "    # We write the extracted data to a CSV-file for easy insertion into PostgreSQL #             \n",
    "    with open(f'/Users/Kevin/Desktop/project_dta/csv_pd2006_2020/{file}.csv', mode='w', newline=\"\") as file_writer:\n",
    "        write = csv.writer(file_writer)\n",
    "        for row in file_ads:\n",
    "            write.writerow(row)\n",
    "    print(f'erroneous ads for {file}: ' + str(error_rows))\n",
    "    print(f'vaild ads for {file}: ' + str(len(file_ads)))\n",
    "    total_valid += len(file_ads); total_errors += error_rows\n",
    "print('\\n')\n",
    "print('total erroneous ads: '+ str(total_errors))\n",
    "print('total valid ads: ' + str(total_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Inserting data into a postgreSQL database and setting up relevant query\n",
    " - To more efficiently analyze the dataset it will be imported postgreSQL.\n",
    " - In the following celles we:\n",
    "    - Connect to the PostgreSQL server\n",
    "    - Create the table and specify column characteristics\n",
    "    - Copy csv files into the table\n",
    "    - Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB created in pgAdmin4 GUI, login and connect cursor #\n",
    "conn = pg2.connect(database='job_ads', user='postgres', password='********')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table creation #\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE historic_ads(\n",
    "    ad_id SERIAL PRIMARY KEY,\n",
    "    headline VARCHAR,\n",
    "    number_of_vacancies INTEGER NOT NULL,\n",
    "    publication_date DATE NOT NULL,\n",
    "    application_deadline DATE NOT NULL);\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing csv files into PostgreSQL #\n",
    "for file in range(2006, 2021):\n",
    "    with open(f'/Users/Kevin/Desktop/project_dta/csv_pd2006_2020/{file}.csv', 'r') as csv_file:\n",
    "        cur.copy_from(csv_file, 'historic_ads', sep=',', columns=('headline', 'number_of_vacancies', 'publication_date', 'application_deadline'))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we check, for each day between 2006 and 2020, how many ads were active the given day and then we sum the number och vacancies in these ads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies_per_day = []\n",
    "for year in range(2006, 2021):\n",
    "    for month in range(1,13): \n",
    "        # cal.monthrange correctly sets the last day for each month #\n",
    "        for day in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "            cur.execute(f\"\"\"\n",
    "SELECT SUM(number_of_vacancies) FROM historic_final_f WHERE '{year}-{month}-{day}' >= publication_date AND '{year}-{month}-{day}' <= application_deadline;\n",
    "\"\"\")\n",
    "            ads_day = [f'{year}-{month}-{day}', cur.fetchone()[0]]\n",
    "            vacancies_per_day.append(ads_day)\n",
    "\n",
    "with open(f'/Users/Kevin/Desktop/project_dta/csv_pd2006_2020/results.csv', mode='w', newline=\"\") as file_writer:\n",
    "    write = csv.writer(file_writer)\n",
    "    for row in vacancies_per_day:\n",
    "        write.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Visualizing and discussion of results\n",
    "#### The comparison data from Statistics Sweden and can be collected [here](https://www.statistikdatabasen.scb.se/pxweb/en/ssd/START__AM__AM0701__AM0701A/KVLedigajobbSektor/).\n",
    "- The comparison data is based on survey results that are collected and published quarterly by Swedens statistical agencie, Statistics Sweden (SCB).\n",
    "- The selected comparison data is between Q1 2006 - Q3 2020 which coincides with our job ads data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in results and comparison .csv and adding appropriate datetime indexing #\n",
    "results = pd.read_csv('/Users/Kevin/Desktop/project_dta/csv_pd2006_2020/results.csv', names=['date', 'vacancies'])\n",
    "results['date'] = pd.to_datetime(results['date'])\n",
    "comparison = pd.read_csv('/Users/Kevin/Desktop/project_dta/csv_pd2006_2020/lediga_jobb.csv')\n",
    "comparison_date = pd.date_range('2006-01-01','2020-12-01', freq='Q')\n",
    "comparison['date'] = comparison_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating range and labels for plot x-axis #\n",
    "date_index = pd.date_range('2006','2022', freq='Y')\n",
    "date_index = date_index.values.astype('datetime64[Y]')\n",
    "date_range = np.arange(2006, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "plt.plot(results.iloc[:,0], results.iloc[:,1], label='Daily labor demand index', color='steelblue')\n",
    "plt.plot(comparison.iloc[:,2], comparison.iloc[:,1], label='Official vacancy survey index', color='indianred')\n",
    "\n",
    "plt.xticks(ticks=date_index, labels=date_range, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/Users/Kevin/Desktop/project_dta/figure_results/results.svg', bbox_inches='tight', facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discission\n",
    "- In the graph above we observe our labor market index plotted together with Statistics Sweden's quarterly survey. Visually, the two time series show a clear co-movement. Thus, we expect that much of the variation in the official survey will be explained by our index.\n",
    "- Given that the daily labor market index is behaving as a higher-frequency version of Statistics Sweden's quarterly survey, it could potentially be used as a cheaper and more timely complement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Statistical analysis - Time series decomposition using Loess (In progress)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
